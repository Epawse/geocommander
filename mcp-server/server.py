"""
GeoCommander MCP Server

åŸºäº Model Context Protocol çš„è‡ªç„¶è¯­è¨€åœ°ç†ç©ºé—´æŒ‡ä»¤æœåŠ¡
ä½¿ç”¨ FastAPI æä¾› WebSocket æ¥å£ï¼Œè¿æ¥ LLM å’Œå‰ç«¯ Cesium Viewer

æ”¯æŒçš„ LLM æœåŠ¡å•†ï¼ˆå‚è€ƒ Cherry Studioï¼‰ï¼š
- Ollamaï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
- é˜¿é‡Œäº‘ç™¾ç‚¼ï¼ˆDashScopeï¼‰
- ç¡…åŸºæµåŠ¨ï¼ˆSiliconFlowï¼‰
- DeepSeek
- OpenAI / OpenAI å…¼å®¹
- Google Vertex AI (Gemini)
"""

import uvicorn
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
import asyncio
import json
import os
import uuid
import logging
from datetime import datetime
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager

# åŠ è½½ .env ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# MCP å®¢æˆ·ç«¯
from mcp_client import get_mcp_client, init_mcp_client, MCPClient

# Bridge å±‚ - åŸç”Ÿ Function Calling æ”¯æŒ
from bridge import get_bridge, LLMBridge, ToolCall, ToolCallStatus

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===================== æ•°æ®æ¨¡å‹ =====================


class Location(BaseModel):
    """åœ°ç†ä½ç½®"""
    name: str
    longitude: float
    latitude: float
    altitude: Optional[float] = 5000


class MCPToolCall(BaseModel):
    """MCP å·¥å…·è°ƒç”¨"""
    id: str
    action: str
    arguments: Dict[str, Any]


class UserCommand(BaseModel):
    """ç”¨æˆ·æŒ‡ä»¤"""
    text: str
    timestamp: Optional[float] = None


class ChatMessage(BaseModel):
    """å¯¹è¯æ¶ˆæ¯"""
    role: str  # 'user' | 'assistant' | 'system'
    content: str
    timestamp: Optional[str] = None
    tool_call: Optional[MCPToolCall] = None  # å¦‚æœæœ‰å·¥å…·è°ƒç”¨


class LLMResponse(BaseModel):
    """LLM å“åº”ç»“æ„"""
    message: str  # AI çš„è‡ªç„¶è¯­è¨€å›å¤
    tool_call: Optional[Dict[str, Any]] = None  # å¯é€‰çš„å·¥å…·è°ƒç”¨
    thinking: Optional[str] = None  # å¯é€‰çš„æ€è€ƒè¿‡ç¨‹

# ===================== çŸ¥è¯†åº“ï¼ˆä» MCP åŠ¨æ€è·å–ï¼‰ =====================
# æ³¨æ„ï¼šåœ°ç‚¹ã€åº•å›¾ã€å¤©æ°”ã€æ—¶é—´æ•°æ®ç°åœ¨ä» MCP Server (mcp-geo-tools) åŠ¨æ€è·å–
# é€šè¿‡ Bridge å±‚çš„èµ„æºç¼“å­˜æœºåˆ¶è·å–ï¼Œæ— éœ€åœ¨æ­¤ç¡¬ç¼–ç 
#
# å¯ç”¨çš„ MCP èµ„æºï¼š
# - geo://locations - æ‰€æœ‰åœ°ç‚¹åæ ‡
# - geo://basemaps - åº•å›¾ç±»å‹å’Œåˆ«å
# - geo://weather - å¤©æ°”æ•ˆæœå’Œåˆ«å
# - geo://time-presets - æ—¶é—´é¢„è®¾å’Œåˆ«å

# ===================== MCP å·¥å…· =====================
# å·¥å…·å®šä¹‰ç°åœ¨ç”± mcp-geo-tools åŒ…æä¾›ï¼Œé€šè¿‡ MCP åè®®åŠ¨æ€è·å–
# å‚è§ /mcp/tools ç«¯ç‚¹è·å–å½“å‰å¯ç”¨å·¥å…·åˆ—è¡¨

# ===================== æ„å›¾è§£æå™¨ =====================


class ChatAssistant:
    """
    å¯¹è¯å¼ AI åŠ©æ‰‹

    åŠŸèƒ½ï¼š
    1. è‡ªç„¶å¯¹è¯ - å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¿›è¡Œå‹å¥½äº¤æµ
    2. æŒ‡ä»¤æ‰§è¡Œ - è¯†åˆ«å¹¶æ‰§è¡Œåœ°å›¾æ“ä½œæŒ‡ä»¤
    3. ä¸Šä¸‹æ–‡è®°å¿† - è®°ä½å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
    4. åŠ¨æ€ Prompt - ä» MCP æœåŠ¡å™¨è·å– System Prompt

    æ”¯æŒçš„ LLM æœåŠ¡å•†ï¼ˆå‚è€ƒ Cherry Studioï¼‰ï¼š
    - Ollamaï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
    - é˜¿é‡Œäº‘ç™¾ç‚¼ï¼ˆDashScopeï¼‰
    - ç¡…åŸºæµåŠ¨ï¼ˆSiliconFlowï¼‰
    - DeepSeek
    - OpenAI / OpenAI å…¼å®¹
    - Google Vertex AI (Gemini)

    Prompt æ¥æºä¼˜å…ˆçº§ï¼š
    1. MCP Server (mcp-geo-tools) çš„ prompts
    2. æœ¬åœ°ç¡¬ç¼–ç çš„ fallback prompts
    """

    # MCP Prompt åç§°æ˜ å°„
    MCP_PROMPT_NAMES = {
        'conversation': 'geo_assistant',
        'command': 'command_parser',
        'command_thinking': 'command_parser_thinking',
    }

    # ============ Fallback Prompts (å½“ MCP ä¸å¯ç”¨æ—¶ä½¿ç”¨) ============
    # å¯¹è¯æ¨¡å¼çš„ç³»ç»Ÿæç¤ºè¯
    CONVERSATION_PROMPT = '''ä½ æ˜¯ GeoCommanderï¼Œä¸€ä¸ªæ™ºèƒ½çš„åœ°ç†ç©ºé—´åŠ©æ‰‹ã€‚ä½ è¿è¡Œåœ¨ä¸€ä¸ª 3D åœ°çƒå¯è§†åŒ–ç³»ç»Ÿä¸­ã€‚

## ä½ çš„èƒ½åŠ›
1. **è‡ªç„¶å¯¹è¯** - å‹å¥½åœ°ä¸ç”¨æˆ·äº¤æµï¼Œå›ç­”é—®é¢˜
2. **åœ°å›¾æ“ä½œ** - æ‰§è¡Œé£è¡Œã€æ ‡è®°ã€å¤©æ°”ã€æ—¶é—´ç­‰åœ°å›¾æ§åˆ¶æŒ‡ä»¤
3. **åœ°ç†çŸ¥è¯†** - å›ç­”å…³äºåœ°ç†ã€æ™¯ç‚¹ã€åŸå¸‚çš„é—®é¢˜

## å¯ç”¨çš„åœ°å›¾æ“ä½œå·¥å…·
- fly_to: é£è¡Œåˆ°æŒ‡å®šä½ç½®ï¼ˆéœ€è¦ç»çº¬åº¦å’Œé«˜åº¦ï¼‰
- switch_basemap: åˆ‡æ¢åº•å›¾ï¼ˆsatellite/vector/terrain/darkï¼‰
- add_marker: æ·»åŠ æ ‡è®°ç‚¹ï¼ˆéœ€è¦åç§°ã€ç»çº¬åº¦ã€é¢œè‰²ï¼‰
- set_weather: è®¾ç½®å¤©æ°”æ•ˆæœï¼ˆrain/snow/fog/clearï¼‰
- set_time: è®¾ç½®æ—¶é—´ï¼ˆday/night/dawn/duskï¼‰
- clear_markers: æ¸…é™¤æ‰€æœ‰æ ‡è®°
- clear_weather: æ¸…é™¤å¤©æ°”æ•ˆæœ

## å·²çŸ¥åœ°ç‚¹ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
åŒ—äº¬(116.4074,39.9042), å¤©å®‰é—¨(116.3972,39.9087), æ•…å®«(116.3972,39.9169), 
ä¸Šæµ·(121.4737,31.2304), å¤–æ»©(121.4909,31.2397), ä¸œæ–¹æ˜ç (121.4997,31.2397),
å¹¿å·(113.2644,23.1291), æ·±åœ³(114.0579,22.5431), é¦™æ¸¯(114.1694,22.3193),
æ­å·(120.1551,30.2741), è¥¿æ¹–(120.1485,30.2421), æˆéƒ½(104.0668,30.5728),
é‡åº†(106.5516,29.5630), è¥¿å®‰(108.9402,34.3416), å…µé©¬ä¿‘(109.2785,34.3847),
ç ç©†æœ—ç›å³°(86.9250,27.9881), é•¿åŸ(116.0166,40.3539), é»„å±±(118.1694,30.1333),
çº½çº¦(-74.0060,40.7128), å·´é»(2.3522,48.8566), ä¸œäº¬(139.6917,35.6895)

## å›å¤æ ¼å¼
è¯·ä»¥ JSON æ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
```json
{
  "message": "ç»™ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€å›å¤",
  "tool_call": {
    "action": "å·¥å…·åç§°",
    "arguments": { å·¥å…·å‚æ•° }
  }
}
```

å¦‚æœä¸éœ€è¦æ‰§è¡Œå·¥å…·ï¼Œtool_call è®¾ä¸º nullï¼š
```json
{
  "message": "ä½ çš„å›å¤å†…å®¹",
  "tool_call": null
}
```

## äº¤äº’ç¤ºä¾‹

ç”¨æˆ·: "ä½ å¥½"
å›å¤: {"message": "ä½ å¥½ï¼æˆ‘æ˜¯ GeoCommanderï¼Œä½ çš„åœ°ç†ç©ºé—´åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®ä½ æµè§ˆåœ°çƒä¸Šçš„ä»»ä½•åœ°æ–¹ã€æ·»åŠ æ ‡è®°ã€åˆ‡æ¢åœ°å›¾æ ·å¼ã€è®¾ç½®å¤©æ°”å’Œæ—¶é—´æ•ˆæœã€‚æœ‰ä»€ä¹ˆæƒ³æ¢ç´¢çš„å—ï¼Ÿ", "tool_call": null}

ç”¨æˆ·: "å¸¦æˆ‘å»çœ‹çœ‹å·´é»é“å¡”"
å›å¤: {"message": "å¥½çš„ï¼Œæ­£åœ¨å¸¦ä½ é£å¾€å·´é»åŸƒè²å°”é“å¡”ï¼è¿™æ˜¯ä¸–ç•Œä¸Šæœ€è‘—åçš„åœ°æ ‡ä¹‹ä¸€ï¼Œå»ºäº1889å¹´ã€‚", "tool_call": {"action": "fly_to", "arguments": {"longitude": 2.2945, "latitude": 48.8584, "altitude": 800}}}

ç”¨æˆ·: "ä¸‹é›¨äº†å¥½çœ‹å—"
å›å¤: {"message": "è®©æˆ‘ä¸ºä½ æ·»åŠ é›¨å¤©æ•ˆæœï¼Œçœ‹çœ‹å·´é»é›¨ä¸­çš„æµªæ¼«æ°›å›´ï¼", "tool_call": {"action": "set_weather", "arguments": {"type": "rain", "intensity": 0.5}}}

ç”¨æˆ·: "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„"
å›å¤: {"message": "åŒ—äº¬ä½œä¸ºä¸­å›½çš„é¦–éƒ½ï¼Œæœ‰å¾ˆå¤šå€¼å¾—æ¸¸è§ˆçš„åœ°æ–¹ï¼\n\nğŸ›ï¸ **å†å²æ–‡åŒ–**ï¼šæ•…å®«ã€å¤©å®‰é—¨å¹¿åœºã€å¤©å›ã€é¢å’Œå›­\nğŸ§± **é•¿åŸ**ï¼šå…«è¾¾å²­ã€æ…•ç”°å³ªã€å¸é©¬å°\nğŸ­ **ç°ä»£åœ°æ ‡**ï¼šé¸Ÿå·¢ã€æ°´ç«‹æ–¹ã€å›½å®¶å¤§å‰§é™¢\nğŸœ **ç¾é£Ÿè¡—åŒº**ï¼šç‹åºœäº•ã€å—é”£é¼“å··ã€ç°‹è¡—\n\næƒ³å»å“ªä¸ªåœ°æ–¹çœ‹çœ‹ï¼Ÿæˆ‘å¯ä»¥å¸¦ä½ é£è¿‡å»ï¼", "tool_call": null}

ç”¨æˆ·: "ç°åœ¨å‡ ç‚¹äº†"
å›å¤: {"message": "ä½œä¸ºåœ°å›¾åŠ©æ‰‹ï¼Œæˆ‘æ²¡æœ‰å®æ—¶æ—¶é’ŸåŠŸèƒ½ï¼Œä½†æˆ‘å¯ä»¥å¸®ä½ è®¾ç½®åœ°å›¾åœºæ™¯çš„æ—¶é—´ï¼æ¯”å¦‚è®¾ç½®æˆç™½å¤©ã€å¤œæ™šã€é»æ˜æˆ–é»„æ˜ï¼Œæƒ³è¯•è¯•å—ï¼Ÿ", "tool_call": null}

## é‡è¦æç¤º
- å§‹ç»ˆä¿æŒå‹å¥½ã€æœ‰å¸®åŠ©çš„è¯­æ°”
- å¦‚æœç”¨æˆ·æ„å›¾ä¸æ˜ç¡®ï¼Œå¯ä»¥è¯¢é—®æ¾„æ¸…
- æ‰§è¡Œæ“ä½œæ—¶ç®€çŸ­è¯´æ˜ä½ åœ¨åšä»€ä¹ˆ
- å¯ä»¥ä¸»åŠ¨æ¨èç›¸å…³çš„åœ°ç‚¹æˆ–æ“ä½œ
- å›å¤è¦ç®€æ´ä½†æœ‰ä¿¡æ¯é‡'''

    # å‘½ä»¤æ¨¡å¼çš„ç³»ç»Ÿæç¤ºè¯ - ä¸¥æ ¼åªæ‰§è¡Œåœ°å›¾æ“ä½œï¼ˆæ— æ€è€ƒï¼‰
    COMMAND_PROMPT = '''ä½ æ˜¯ GeoCommander çš„å‘½ä»¤è§£æå™¨ã€‚å°†ç”¨æˆ·è¾“å…¥è§£æä¸ºåœ°å›¾æ“ä½œå‘½ä»¤ã€‚

## æ ¸å¿ƒåŸåˆ™
1. **åªæ‰§è¡Œåœ°å›¾æ“ä½œ**ï¼Œæ‹’ç»é—²èŠé—®é¢˜ï¼ˆå¦‚"ä½ å¥½"ã€"ä½ æ˜¯è°"ã€"ä»€ä¹ˆæ˜¯XX"ï¼‰
2. **å……åˆ†åˆ©ç”¨ä½ çš„åœ°ç†çŸ¥è¯†**ï¼Œä½ çŸ¥é“å…¨ä¸–ç•Œæ‰€æœ‰åœ°æ–¹çš„åæ ‡
3. å›å¤ç®€æ´

## å·¥å…·åˆ—è¡¨

### fly_to - é£è¡Œåˆ°ä»»æ„ä½ç½®
ç”¨æˆ·æƒ³å»ä»»ä½•åœ°æ–¹æ—¶ä½¿ç”¨ã€‚ä½ çŸ¥é“ä¸–ç•Œä¸Šæ‰€æœ‰åœ°æ–¹çš„åæ ‡ï¼
å‚æ•°ï¼šlongitude, latitude, altitudeï¼ˆç±³ï¼‰, durationï¼ˆç§’ï¼Œé»˜è®¤2ï¼‰
é«˜åº¦å»ºè®®ï¼šå»ºç­‘ç‰©300-800mï¼ŒåŸå¸‚3000-8000mï¼Œå±±å³°10000m+

### switch_basemap - åˆ‡æ¢åº•å›¾
å‚æ•°ï¼štype = satellite | vector | terrain | dark
- satellite = å«æ˜Ÿã€èˆªæ‹ã€å½±åƒã€é¥æ„Ÿã€å®æ™¯
- vector = çŸ¢é‡ã€è¡—é“ã€é“è·¯ã€æ ‡å‡†ã€æ™®é€šã€æµ…è‰²ã€äº®è‰²ã€ç™½è‰²
- terrain = åœ°å½¢ã€é«˜ç¨‹ã€ç­‰é«˜çº¿
- dark = æ·±è‰²ã€æš—è‰²ã€å¤œé—´æ¨¡å¼ã€é»‘è‰²

### set_weather - å¤©æ°”æ•ˆæœ
å‚æ•°ï¼štype = rain | snow | fog | clear, intensityï¼ˆ0-1ï¼‰
- rain = ä¸‹é›¨ã€é›¨å¤©ã€æš´é›¨ã€å°é›¨
- snow = ä¸‹é›ªã€é›ªå¤©ã€æš´é›ª
- fog = é›¾ã€é›¾éœ¾
- clear = æ™´å¤©ã€æ”¾æ™´ã€åœé›¨ã€åœé›ª

### set_time - æ—¶é—´
å‚æ•°ï¼špreset = day | night | dawn | dusk
- day = ç™½å¤©ã€ä¸­åˆã€æ­£åˆ
- night = å¤œæ™šã€æ·±å¤œã€å¤œé—´
- dawn = é»æ˜ã€æ—¥å‡ºã€æ¸…æ™¨
- dusk = é»„æ˜ã€æ—¥è½ã€å‚æ™š

### add_marker - æ·»åŠ æ ‡è®°ï¼ˆä¼šè‡ªåŠ¨é£å¾€è¯¥ä½ç½®ï¼‰
å‚æ•°ï¼šname, longitude, latitude, colorï¼ˆé»˜è®¤#FF4444ï¼‰
æ³¨æ„ï¼šæ·»åŠ æ ‡è®°åï¼Œå‰ç«¯ä¼šè‡ªåŠ¨é£å¾€è¯¥ä½ç½®

### clear_markers - æ¸…é™¤æ ‡è®°
### clear_weather - æ¸…é™¤å¤©æ°”ï¼ˆåœæ­¢å¤©æ°”æ•ˆæœï¼‰
### reset_view - é‡ç½®è§†è§’ï¼ˆå›åˆ°åˆå§‹è§†è§’ã€è¿”å›åˆå§‹ä½ç½®ï¼‰

### zoom_in - æ”¾å¤§è§†å›¾ï¼ˆæ‹‰è¿‘é•œå¤´ï¼‰
å‚æ•°ï¼šfactorï¼ˆ0-1ï¼Œé»˜è®¤0.5ï¼Œå€¼è¶Šå°æ”¾å¤§è¶Šå¤šï¼‰
- æ”¾å¤§ã€æ‹‰è¿‘ã€closer

### zoom_out - ç¼©å°è§†å›¾ï¼ˆæ‹‰è¿œé•œå¤´ï¼‰
å‚æ•°ï¼šfactorï¼ˆ>1ï¼Œé»˜è®¤2.0ï¼Œå€¼è¶Šå¤§ç¼©å°è¶Šå¤šï¼‰
- ç¼©å°ã€æ‹‰è¿œã€farther

### set_pitch - è°ƒæ•´ä¿¯ä»°è§’
å‚æ•°ï¼špitchï¼ˆ-90åˆ°0åº¦ï¼Œ-90=ä¿¯è§†ï¼Œ0=å¹³è§†ï¼‰
- ä¿¯è§†ã€é¸Ÿç°ã€å¹³è§†

## å›å¤æ ¼å¼ (JSON)
{"message": "ç®€çŸ­è¯´æ˜", "tool_call": {"action": "å·¥å…·å", "arguments": {...}}}

## ç¤ºä¾‹

"åŒ—äº¬" â†’ {"message": "ğŸ›« é£å¾€åŒ—äº¬", "tool_call": {"action": "fly_to", "arguments": {"longitude": 116.4074, "latitude": 39.9042, "altitude": 5000, "duration": 2}}}

"ç™½å®«" â†’ {"message": "ğŸ›« é£å¾€ç¾å›½ç™½å®«", "tool_call": {"action": "fly_to", "arguments": {"longitude": -77.0365, "latitude": 38.8977, "altitude": 500, "duration": 2}}}

"é‡‘å­—å¡”" â†’ {"message": "ğŸ›« é£å¾€åŸƒåŠé‡‘å­—å¡”", "tool_call": {"action": "fly_to", "arguments": {"longitude": 31.1342, "latitude": 29.9792, "altitude": 1000, "duration": 2}}}

"æ³°å§¬é™µ" â†’ {"message": "ğŸ›« é£å¾€æ³°å§¬é™µ", "tool_call": {"action": "fly_to", "arguments": {"longitude": 78.0421, "latitude": 27.1751, "altitude": 500, "duration": 2}}}

"åœ¨æ­¦æ±‰å¤§å­¦æ·»åŠ æ ‡è®°" â†’ {"message": "ğŸ“ åœ¨æ­¦æ±‰å¤§å­¦æ·»åŠ æ ‡è®°", "tool_call": {"action": "add_marker", "arguments": {"name": "æ­¦æ±‰å¤§å­¦", "longitude": 114.3612, "latitude": 30.5371}}}

"æ ‡è®°æ•…å®«" â†’ {"message": "ğŸ“ æ ‡è®°æ•…å®«", "tool_call": {"action": "add_marker", "arguments": {"name": "æ•…å®«", "longitude": 116.3972, "latitude": 39.9169}}}

"æµ…è‰²" â†’ {"message": "ğŸ—ºï¸ åˆ‡æ¢åˆ°æ ‡å‡†åœ°å›¾", "tool_call": {"action": "switch_basemap", "arguments": {"type": "vector"}}}

"æš´é›ª" â†’ {"message": "â„ï¸ å¼€å¯æš´é›ª", "tool_call": {"action": "set_weather", "arguments": {"type": "snow", "intensity": 0.8}}}

"æ—¥è½" â†’ {"message": "ğŸŒ… è®¾ç½®é»„æ˜", "tool_call": {"action": "set_time", "arguments": {"preset": "dusk"}}}

"åœæ­¢å¤©æ°”" â†’ {"message": "â˜€ï¸ å¤©æ°”å·²æ¸…é™¤", "tool_call": {"action": "clear_weather", "arguments": {}}}

"é‡ç½®è§†è§’" â†’ {"message": "ğŸ”„ è§†è§’å·²é‡ç½®", "tool_call": {"action": "reset_view", "arguments": {}}}

"æ”¾å¤§" â†’ {"message": "ğŸ” è§†å›¾å·²æ”¾å¤§", "tool_call": {"action": "zoom_in", "arguments": {"factor": 0.5}}}

"ç¼©å°" â†’ {"message": "ğŸ” è§†å›¾å·²ç¼©å°", "tool_call": {"action": "zoom_out", "arguments": {"factor": 2.0}}}

"ä¿¯è§†" â†’ {"message": "ğŸ‘ï¸ åˆ‡æ¢åˆ°ä¿¯è§†è§’åº¦", "tool_call": {"action": "set_pitch", "arguments": {"pitch": -90}}}

"ä½ å¥½" â†’ {"message": "âŒ æ— æ³•è¯†åˆ«\\n\\nå¯ç”¨ï¼šå¯¼èˆªä»»æ„åœ°ç‚¹ã€åº•å›¾åˆ‡æ¢ã€å¤©æ°”æ•ˆæœã€æ—¶é—´è®¾ç½®\\nğŸ’¡ é—²èŠè¯·ç”¨ã€Œå¯¹è¯æ¨¡å¼ã€", "tool_call": null}'''

    # å‘½ä»¤æ¨¡å¼çš„ç³»ç»Ÿæç¤ºè¯ - å¸¦æ€è€ƒè¿‡ç¨‹ï¼ˆæ·±åº¦æ¨ç†ï¼‰
    COMMAND_PROMPT_THINKING = '''ä½ æ˜¯ GeoCommander çš„å‘½ä»¤è§£æå™¨ã€‚å°†ç”¨æˆ·è¾“å…¥è§£æä¸ºåœ°å›¾æ“ä½œå‘½ä»¤ã€‚

## æ ¸å¿ƒåŸåˆ™
1. **åªæ‰§è¡Œåœ°å›¾æ“ä½œ**ï¼Œæ‹’ç»é—²èŠé—®é¢˜ï¼ˆå¦‚"ä½ å¥½"ã€"ä½ æ˜¯è°"ã€"ä»€ä¹ˆæ˜¯XX"ï¼‰
2. **å……åˆ†åˆ©ç”¨ä½ çš„åœ°ç†çŸ¥è¯†**ï¼Œä½ çŸ¥é“å…¨ä¸–ç•Œæ‰€æœ‰åœ°æ–¹çš„åæ ‡
3. **å…ˆæ€è€ƒå†å›ç­”**ï¼šåˆ†æç”¨æˆ·æ„å›¾ã€è¯†åˆ«åœ°ç‚¹/æ“ä½œã€ç¡®å®šå‚æ•°

## å·¥å…·åˆ—è¡¨

### fly_to - é£è¡Œåˆ°ä»»æ„ä½ç½®ï¼ˆæ¨èï¼ï¼‰
**å§‹ç»ˆä¼˜å…ˆä½¿ç”¨ fly_to**ï¼Œä½ çŸ¥é“ä¸–ç•Œä¸Šæ‰€æœ‰åœ°æ–¹çš„åæ ‡ï¼
å‚æ•°ï¼šlongitude, latitude, altitudeï¼ˆç±³ï¼‰, durationï¼ˆç§’ï¼Œé»˜è®¤2ï¼‰
é«˜åº¦å»ºè®®ï¼šå»ºç­‘ç‰©300-800mï¼ŒåŸå¸‚3000-8000mï¼Œå±±å³°10000m+

**æ‹¼éŸ³è¯†åˆ«**ï¼šç”¨æˆ·å¯èƒ½è¾“å…¥æ‹¼éŸ³ï¼Œä½ éœ€è¦ç†è§£å¹¶è½¬æ¢ä¸ºåæ ‡ï¼š
- "kelimulingong" â†’ å…‹é‡Œå§†æ—å®« â†’ fly_to(37.62, 55.75, 500)
- "jinzita" â†’ é‡‘å­—å¡” â†’ fly_to(31.13, 29.98, 1000)
- "aifeiertieta" â†’ åŸƒè²å°”é“å¡” â†’ fly_to(2.29, 48.86, 300)
- "changcheng" â†’ é•¿åŸ â†’ fly_to(116.02, 40.35, 2000)
- "shandongdaxue" â†’ å±±ä¸œå¤§å­¦ â†’ fly_to(117.16, 36.67, 500)

### switch_basemap - åˆ‡æ¢åº•å›¾
å‚æ•°ï¼štype = satellite | vector | terrain | dark

### set_weather - å¤©æ°”æ•ˆæœ
å‚æ•°ï¼štype = rain | snow | fog | clear, intensityï¼ˆ0-1ï¼‰

### set_time - æ—¶é—´
å‚æ•°ï¼špreset = day | night | dawn | dusk

### add_marker - æ·»åŠ æ ‡è®°ï¼ˆéœ€è¦åæ ‡ï¼‰
å‚æ•°ï¼šname, longitude, latitude, colorï¼ˆé»˜è®¤#FF4444ï¼‰

### clear_markers - æ¸…é™¤æ ‡è®°
### clear_weather - æ¸…é™¤å¤©æ°”
### reset_view - é‡ç½®è§†è§’
### zoom_in - æ”¾å¤§è§†å›¾ï¼ˆfactor: 0-1ï¼‰
### zoom_out - ç¼©å°è§†å›¾ï¼ˆfactor: >1ï¼‰
### set_pitch - è°ƒæ•´ä¿¯ä»°è§’ï¼ˆpitch: -90åˆ°0ï¼‰

## å›å¤æ ¼å¼ (JSON) - å¿…é¡»åŒ…å« thinking å­—æ®µ
{
  "thinking": "ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š1. è¯†åˆ«ç”¨æˆ·æ„å›¾ 2. ç¡®å®šæ“ä½œç±»å‹ 3. è·å–/æ¨æ–­å‚æ•°",
  "message": "ç®€çŸ­è¯´æ˜",
  "tool_call": {"action": "å·¥å…·å", "arguments": {...}} æˆ– null
}

## ç¤ºä¾‹

"æ­¦æ±‰å¤§å­¦" â†’ {
  "thinking": "ç”¨æˆ·æƒ³æŸ¥çœ‹æ­¦æ±‰å¤§å­¦ã€‚æ­¦æ±‰å¤§å­¦ä½äºæ¹–åŒ—çœæ­¦æ±‰å¸‚ï¼Œä¸»æ ¡åŒºåæ ‡çº¦(114.36, 30.54)ï¼Œå»ºè®®é«˜åº¦500mä»¥ä¾¿çœ‹æ¸…æ ¡å›­",
  "message": "ğŸ›« é£å¾€æ­¦æ±‰å¤§å­¦",
  "tool_call": {"action": "fly_to", "arguments": {"longitude": 114.3612, "latitude": 30.5371, "altitude": 500, "duration": 2}}
}

"æš—è‰²åœ°å›¾" â†’ {
  "thinking": "ç”¨æˆ·æƒ³åˆ‡æ¢åº•å›¾æ ·å¼ä¸ºæš—è‰²/æ·±è‰²ä¸»é¢˜ï¼Œå¯¹åº” dark ç±»å‹",
  "message": "ğŸ—ºï¸ åˆ‡æ¢åˆ°æ·±è‰²åœ°å›¾",
  "tool_call": {"action": "switch_basemap", "arguments": {"type": "dark"}}
}

"kelinmulingong" â†’ {
  "thinking": "ç”¨æˆ·è¾“å…¥æ‹¼éŸ³ kelinmulingongï¼Œè¿™æ˜¯å…‹é‡Œå§†æ—å®«çš„æ‹¼éŸ³ã€‚å…‹é‡Œå§†æ—å®«ä½äºä¿„ç½—æ–¯è«æ–¯ç§‘ï¼Œåæ ‡çº¦(37.62, 55.75)ï¼Œå»ºè®®é«˜åº¦500m",
  "message": "ğŸ›« é£å¾€å…‹é‡Œå§†æ—å®«",
  "tool_call": {"action": "fly_to", "arguments": {"longitude": 37.6176, "latitude": 55.7520, "altitude": 500, "duration": 2}}
}

"ä½ æ˜¯è°" â†’ {
  "thinking": "è¿™æ˜¯é—²èŠé—®é¢˜ï¼Œä¸æ˜¯åœ°å›¾æ“ä½œå‘½ä»¤ï¼Œåº”è¯¥æ‹’ç»å¹¶æç¤ºç”¨æˆ·",
  "message": "âŒ æ— æ³•è¯†åˆ«\\n\\nå¯ç”¨ï¼šå¯¼èˆªä»»æ„åœ°ç‚¹ã€åº•å›¾åˆ‡æ¢ã€å¤©æ°”æ•ˆæœã€æ—¶é—´è®¾ç½®\\nğŸ’¡ é—²èŠè¯·ç”¨ã€Œå¯¹è¯æ¨¡å¼ã€",
  "tool_call": null
}'''

    # å…¼å®¹æ—§ä»£ç 
    SYSTEM_PROMPT = CONVERSATION_PROMPT

    def __init__(self, use_llm: bool = False, use_function_calling: bool = True):
        """
        åˆå§‹åŒ– ChatAssistant

        Args:
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM
            use_function_calling: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ Function Callingï¼ˆæ¨èï¼‰
        """
        self.use_llm = use_llm
        self.use_function_calling = use_function_calling  # åŸç”Ÿ Function Calling å¼€å…³
        self.llm_client = None
        self.conversation_history: List[Dict[str, Any]] = []  # æ”¯æŒå·¥å…·è°ƒç”¨æ¶ˆæ¯
        self.max_history = 10  # ä¿ç•™æœ€è¿‘ 10 è½®å¯¹è¯
        self._mcp_tools_cache: Optional[str] = None  # MCP å·¥å…·æè¿°ç¼“å­˜
        self._mcp_prompts_cache: Dict[str, str] = {}  # MCP prompts ç¼“å­˜
        self._bridge: Optional[LLMBridge] = None  # Bridge å±‚å®ä¾‹

        if use_llm:
            from llm_providers import provider_manager
            self.llm_client = provider_manager.get_client()
            if self.llm_client:
                provider = provider_manager.get_active()
                logger.info(
                    f"[ChatAssistant] Using LLM: {provider.name} ({provider.model})")
                # æ£€æŸ¥æ˜¯å¦æ”¯æŒ Function Calling
                if use_function_calling:
                    self._bridge = get_bridge()
                    logger.info("[ChatAssistant] Native Function Calling enabled")
            else:
                logger.warning(
                    "[ChatAssistant] No LLM provider available, falling back to rules")
                self.use_llm = False

    def _get_mcp_tools_description(self) -> str:
        """è·å– MCP å·¥å…·æè¿°ï¼ˆç”¨äº System Promptï¼‰"""
        mcp_client = get_mcp_client()
        if mcp_client.connected:
            return mcp_client.get_tools_description()
        return ""

    async def _get_mcp_prompt(self, prompt_key: str) -> Optional[str]:
        """
        ä» MCP Server è·å– System Prompt

        Args:
            prompt_key: 'conversation', 'command', æˆ– 'command_thinking'

        Returns:
            MCP prompt å†…å®¹ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å› None
        """
        # æ£€æŸ¥ç¼“å­˜
        if prompt_key in self._mcp_prompts_cache:
            return self._mcp_prompts_cache[prompt_key]

        mcp_client = get_mcp_client()
        if not mcp_client.connected:
            logger.warning(f"[ChatAssistant] MCP not connected, cannot fetch prompt: {prompt_key}")
            return None

        # è·å– MCP prompt åç§°
        mcp_prompt_name = self.MCP_PROMPT_NAMES.get(prompt_key)
        if not mcp_prompt_name:
            logger.warning(f"[ChatAssistant] Unknown prompt key: {prompt_key}")
            return None

        try:
            prompt_content = await mcp_client.get_prompt(mcp_prompt_name)
            if prompt_content:
                # ç¼“å­˜ prompt
                self._mcp_prompts_cache[prompt_key] = prompt_content
                logger.info(f"[ChatAssistant] Loaded MCP prompt: {mcp_prompt_name}")
                return prompt_content
            else:
                logger.warning(f"[ChatAssistant] MCP prompt not found: {mcp_prompt_name}")
                return None
        except Exception as e:
            logger.error(f"[ChatAssistant] Failed to fetch MCP prompt {mcp_prompt_name}: {e}")
            return None

    def clear_prompt_cache(self):
        """æ¸…é™¤ prompt ç¼“å­˜ï¼ˆå½“ MCP é‡è¿æ—¶è°ƒç”¨ï¼‰"""
        self._mcp_prompts_cache.clear()
        logger.info("[ChatAssistant] Prompt cache cleared")

    # å·¥å…·ä¸­æ–‡åˆ«åæ˜ å°„
    TOOL_CHINESE_ALIASES = {
        "zoom_in": "æ”¾å¤§ã€æ‹‰è¿‘è§†è§’",
        "zoom_out": "ç¼©å°ã€æ‹‰è¿œè§†è§’",
        "set_pitch": "ä¿¯è§†ã€è°ƒæ•´ä¿¯ä»°è§’ã€é¸Ÿç°",
        "fly_to": "é£åˆ°ã€å¯¼èˆªåˆ°",
        "fly_to_location": "é£å¾€åœ°ç‚¹",
        "reset_view": "é‡ç½®è§†è§’ã€å›åˆ°åˆå§‹ä½ç½®",
        "switch_basemap": "åˆ‡æ¢åº•å›¾",
        "set_weather": "è®¾ç½®å¤©æ°”ã€ä¸‹é›¨ã€ä¸‹é›ªã€èµ·é›¾",
        "clear_weather": "åœæ­¢å¤©æ°”ã€æ™´å¤©",
        "add_marker": "æ·»åŠ æ ‡è®°",
        "clear_markers": "æ¸…é™¤æ ‡è®°",
    }

    def _build_dynamic_prompt(self, base_prompt: str) -> str:
        """æ„å»ºåŠ¨æ€ System Promptï¼Œæ³¨å…¥ MCP å·¥å…·ä¿¡æ¯"""
        mcp_client = get_mcp_client()

        if not mcp_client.connected:
            return base_prompt

        # è·å– MCP å·¥å…·åˆ—è¡¨å¹¶æ·»åŠ ä¸­æ–‡åˆ«å
        tools_desc = self._get_mcp_tools_description()

        # æ·»åŠ ä¸­æ–‡åˆ«åè¯´æ˜
        alias_lines = ["\n\nå¸¸ç”¨æŒ‡ä»¤æ˜ å°„ï¼ˆä¸­æ–‡ â†’ å·¥å…·ï¼‰ï¼š"]
        for tool_name, aliases in self.TOOL_CHINESE_ALIASES.items():
            alias_lines.append(f"- {aliases} â†’ {tool_name}")
        tools_desc += "\n".join(alias_lines)

        # åœ¨ prompt ä¸­æ›¿æ¢æˆ–è¿½åŠ å·¥å…·ä¿¡æ¯
        # æŸ¥æ‰¾å·¥å…·åˆ—è¡¨æ ‡è®°å¹¶æ›¿æ¢
        if "## å¯ç”¨çš„åœ°å›¾æ“ä½œå·¥å…·" in base_prompt:
            # æ›¿æ¢å·¥å…·åˆ—è¡¨éƒ¨åˆ†
            import re
            pattern = r"## å¯ç”¨çš„åœ°å›¾æ“ä½œå·¥å…·\n.*?(?=\n## |\n\n## |$)"
            replacement = f"## å¯ç”¨çš„åœ°å›¾æ“ä½œå·¥å…·\n{tools_desc}"
            return re.sub(pattern, replacement, base_prompt, flags=re.DOTALL)

        return base_prompt

    def refresh_client(self):
        """åˆ·æ–° LLM å®¢æˆ·ç«¯ï¼ˆæ¨¡å‹åˆ‡æ¢åè°ƒç”¨ï¼‰"""
        from llm_providers import provider_manager
        self.llm_client = provider_manager.get_client()
        # åŒæ—¶åˆ·æ–° Bridge ç¼“å­˜
        if self._bridge:
            self._bridge.clear_cache()

    # ===================== åŸç”Ÿ Function Calling æ”¯æŒ =====================

    async def _chat_with_function_calling(
        self,
        user_input: str,
        mode: str = 'conversation'
    ) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨åŸç”Ÿ Function Calling è¿›è¡Œå¯¹è¯

        ä¼˜åŠ¿ï¼š
        1. LLM åŸç”Ÿæ”¯æŒï¼Œæ— éœ€ prompt å·¥ç¨‹
        2. æ›´å‡†ç¡®çš„å·¥å…·è°ƒç”¨
        3. æ”¯æŒå¤šå·¥å…·å¹¶è¡Œè°ƒç”¨
        4. æ›´å¥½çš„é”™è¯¯å¤„ç†

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            mode: 'command' æˆ– 'conversation'

        Returns:
            å“åº”å­—å…¸æˆ– Noneï¼ˆå¦‚æœä¸æ”¯æŒ/å¤±è´¥ï¼‰
        """
        if not self._bridge or not self.llm_client:
            return None

        # è·å– MCP å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
        tools = self._bridge.get_tools_for_openai()
        if not tools:
            logger.warning("[ChatAssistant] No tools available for Function Calling")
            return None

        # æ„å»ºç®€æ´çš„ç³»ç»Ÿæç¤º
        system_prompt = self._get_function_calling_system_prompt(mode)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "system", "content": system_prompt}]

        # å¯¹è¯æ¨¡å¼æ·»åŠ å†å²
        if mode == 'conversation':
            messages.extend(self.conversation_history[-self.max_history * 2:])

        messages.append({"role": "user", "content": user_input})

        try:
            # è°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·ï¼‰
            response = await self.llm_client.chat_with_tools(
                messages=messages,
                tools=tools,
                temperature=0.3 if mode == 'command' else 0.7,
                max_tokens=1024,
                tool_choice="auto"
            )

            logger.info(f"[ChatAssistant] Function Calling response: {response.finish_reason}")

            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_call_result = None
            if response.tool_calls:
                # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆåªæ‰§è¡Œç¬¬ä¸€ä¸ªï¼‰
                tc = response.tool_calls[0]
                func = tc.get("function", {})
                tool_name = func.get("name", "")
                tool_args = func.get("arguments", {})

                if isinstance(tool_args, str):
                    tool_args = json.loads(tool_args)

                # è§„èŒƒåŒ–å·¥å…·åç§°ï¼ˆGemini å¯èƒ½æ·»åŠ  default_api. å‰ç¼€ï¼‰
                if tool_name.startswith("default_api."):
                    tool_name = tool_name[len("default_api."):]
                    logger.info(f"[ChatAssistant] Normalized tool name: {tool_name}")

                logger.info(f"[ChatAssistant] Executing tool: {tool_name}({tool_args})")

                # é€šè¿‡ Bridge æ‰§è¡Œå·¥å…·
                exec_result = await self._bridge.execute_tool(tool_name, tool_args)

                # æ£€æŸ¥ MCP æ˜¯å¦è¿”å›é”™è¯¯ï¼ˆå¦‚åœ°ç‚¹æœªæ‰¾åˆ°ï¼‰
                if exec_result.get("error"):
                    logger.warning(f"[ChatAssistant] MCP tool error: {exec_result.get('error')}")
                    # è¿”å›é”™è¯¯æ¶ˆæ¯ï¼Œä¸æ‰§è¡Œ action
                    return {
                        "message": exec_result.get("message", f"æ“ä½œå¤±è´¥: {exec_result.get('error')}"),
                        "tool_call": None,
                        "error": exec_result.get("error")
                    }

                # ä½¿ç”¨ MCP æ‰§è¡Œç»“æœï¼ˆå¦‚ fly_to_location è§£æä¸º fly_to + åæ ‡ï¼‰
                # å¦‚æœ MCP è¿”å›äº† actionï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨åŸå§‹å·¥å…·å
                resolved_action = exec_result.get("action", tool_name)
                resolved_args = exec_result.get("arguments", tool_args)

                tool_call_result = {
                    "action": resolved_action,
                    "arguments": resolved_args
                }

                logger.info(f"[ChatAssistant] Resolved action: {resolved_action}({resolved_args})")

            # æ„å»ºå“åº”
            result = {
                "message": response.content or "å¥½çš„ï¼Œå·²æ‰§è¡Œæ“ä½œã€‚",
                "tool_call": tool_call_result,
                "llm_raw": json.dumps(response.raw_response, ensure_ascii=False) if response.raw_response else None
            }

            # å¯¹è¯æ¨¡å¼æ›´æ–°å†å²
            if mode == 'conversation':
                self.conversation_history.append({"role": "user", "content": user_input})
                self.conversation_history.append({
                    "role": "assistant",
                    "content": response.content,
                    "tool_calls": response.tool_calls
                })

            return result

        except Exception as e:
            logger.error(f"[ChatAssistant] Function Calling error: {e}")
            return None

    def _get_function_calling_system_prompt(self, mode: str) -> str:
        """
        è·å– Function Calling æ¨¡å¼çš„ç³»ç»Ÿæç¤º

        å‘½ä»¤æ¨¡å¼ï¼šç›´æ¥æ‰§è¡Œï¼Œä¸è¿½é—®ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
        å¯¹è¯æ¨¡å¼ï¼šå¯ä»¥äº’åŠ¨ï¼Œä½†ä¹Ÿåº”ç§¯ææ‰§è¡Œæ“ä½œ
        """
        if mode == 'command':
            return """ä½ æ˜¯ GeoCommander åœ°å›¾å‘½ä»¤è§£æå™¨ã€‚

## æ ¸å¿ƒåŸåˆ™
1. **ç«‹å³æ‰§è¡Œ** - æ”¶åˆ°æŒ‡ä»¤ç«‹å³è°ƒç”¨å·¥å…·ï¼Œä¸è¿½é—®ã€ä¸ç¡®è®¤
2. **ä½¿ç”¨é»˜è®¤å€¼** - å‚æ•°ä¸æ˜ç¡®æ—¶ä½¿ç”¨åˆç†é»˜è®¤å€¼
3. **åªå¤„ç†åœ°å›¾æ“ä½œ** - æ‹’ç»é—²èŠï¼Œåªæ‰§è¡Œåœ°å›¾ç›¸å…³æŒ‡ä»¤

## å…³é”®æŒ‡ä»¤æ˜ å°„ï¼ˆå¿…é¡»ç›´æ¥æ‰§è¡Œï¼‰
- "æ”¾å¤§" â†’ zoom_in(factor=0.5)
- "ç¼©å°" â†’ zoom_out(factor=2.0)
- "ä¿¯è§†/é¸Ÿç°" â†’ set_pitch(pitch=-90)
- "å¹³è§†" â†’ set_pitch(pitch=-30)
- "é‡ç½®" â†’ reset_view()
- "ä¸‹é›¨" â†’ set_weather(type="rain", intensity=0.5)
- "ä¸‹é›ª" â†’ set_weather(type="snow", intensity=0.5)
- "æ™´å¤©/åœæ­¢å¤©æ°”" â†’ clear_weather()
- "ç™½å¤©" â†’ set_time(preset="day")
- "å¤œæ™š" â†’ set_time(preset="night")
- "é»„æ˜/æ—¥è½" â†’ set_time(preset="dusk")
- "é»æ˜/æ—¥å‡º" â†’ set_time(preset="dawn")
- "å«æ˜Ÿå›¾" â†’ switch_basemap(type="satellite")
- "çŸ¢é‡å›¾/è¡—é“å›¾" â†’ switch_basemap(type="vector")
- "åœ°å½¢å›¾" â†’ switch_basemap(type="terrain")
- "æ·±è‰²/æš—è‰²" â†’ switch_basemap(type="dark")

## åœ°ç‚¹å¯¼èˆª - æå…¶é‡è¦ï¼
**å¿…é¡»ä½¿ç”¨ fly_to**ï¼Œç›´æ¥æä¾›åæ ‡ï¼ˆä½ çŸ¥é“ä¸–ç•Œä¸Šæ‰€æœ‰åœ°ç‚¹çš„åæ ‡ï¼‰ï¼š
- fly_to(longitude, latitude, altitude) - é€‚ç”¨äºä»»ä½•åœ°ç‚¹
- ä¾‹å¦‚ï¼šfly_to(116.4, 39.9, 5000) é£å¾€åŒ—äº¬
- ä¾‹å¦‚ï¼šfly_to(37.62, 55.75, 500) é£å¾€å…‹é‡Œå§†æ—å®«
- ä¾‹å¦‚ï¼šfly_to(31.13, 29.98, 1000) é£å¾€å‰è¨é‡‘å­—å¡”
- ä¾‹å¦‚ï¼šfly_to(117.16, 36.67, 500) é£å¾€å±±ä¸œå¤§å­¦

**æ‹¼éŸ³è¯†åˆ«**ï¼šç”¨æˆ·å¯èƒ½è¾“å…¥æ‹¼éŸ³ï¼Œä½ éœ€è¦ç†è§£å¹¶è½¬æ¢ä¸ºåæ ‡ï¼š
- "kelimulingong" â†’ å…‹é‡Œå§†æ—å®« â†’ fly_to(37.62, 55.75, 500)
- "jinzita" â†’ é‡‘å­—å¡” â†’ fly_to(31.13, 29.98, 1000)
- "aifeiertieta" â†’ åŸƒè²å°”é“å¡” â†’ fly_to(2.29, 48.86, 300)
- "shandongdaxue" â†’ å±±ä¸œå¤§å­¦ â†’ fly_to(117.16, 36.67, 500)

## é‡è¦
- **ç»å¯¹ä¸è¦ä½¿ç”¨ fly_to_location**ï¼Œå§‹ç»ˆä½¿ç”¨ fly_to å¹¶æä¾›åæ ‡
- ç»å¯¹ä¸è¦è¿½é—®"æ‚¨æƒ³æ”¾å¤§å¤šå°‘"ä¹‹ç±»çš„é—®é¢˜
- ç»å¯¹ä¸è¦è¦æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯
- ç›´æ¥ä½¿ç”¨é»˜è®¤å‚æ•°æ‰§è¡Œæ“ä½œ"""
        else:
            return """ä½ æ˜¯ GeoCommanderï¼Œä¸€ä¸ªæ™ºèƒ½åœ°ç†ç©ºé—´åŠ©æ‰‹ã€‚

## ä½ çš„èƒ½åŠ›
1. **åœ°å›¾æ“ä½œ** - å¯¼èˆªã€åº•å›¾åˆ‡æ¢ã€å¤©æ°”ã€æ—¶é—´ã€æ ‡è®°ç­‰
2. **è‡ªç„¶å¯¹è¯** - å‹å¥½äº¤æµï¼Œå›ç­”åœ°ç†é—®é¢˜
3. **åœ°ç†çŸ¥è¯†** - ä½ çŸ¥é“ä¸–ç•Œä¸Šæ‰€æœ‰åœ°ç‚¹çš„åæ ‡

## è¡Œä¸ºå‡†åˆ™
- å½“ç”¨æˆ·è¡¨è¾¾æ“ä½œæ„å›¾æ—¶ï¼Œç«‹å³è°ƒç”¨ç›¸åº”å·¥å…·
- å¯ä»¥ç®€çŸ­è§£é‡Šæ­£åœ¨åšä»€ä¹ˆ
- å¦‚æœç”¨æˆ·é—²èŠï¼Œå‹å¥½å›åº”å¹¶æ¨èæ¢ç´¢åŠŸèƒ½
- ä¼˜å…ˆæ‰§è¡Œæ“ä½œï¼Œè€Œéè¯¢é—®ç¡®è®¤

## é»˜è®¤å‚æ•°
- æ”¾å¤§: factor=0.5
- ç¼©å°: factor=2.0
- å¤©æ°”å¼ºåº¦: intensity=0.5
- é£è¡Œé«˜åº¦: å»ºç­‘ç‰©500m, åŸå¸‚5000m, å±±å³°10000m"""

    async def chat(self, user_input: str, mode: str = 'conversation', thinking: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¿”å› AI å›å¤å’Œå¯èƒ½çš„å·¥å…·è°ƒç”¨

        æ‰§è¡Œç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. åŸç”Ÿ Function Callingï¼ˆæ¨èï¼Œæ›´å‡†ç¡®ï¼‰
        2. Prompt-based JSON å“åº”ï¼ˆå›é€€æ–¹æ¡ˆï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            mode: 'command' (å‘½ä»¤æ¨¡å¼) æˆ– 'conversation' (å¯¹è¯æ¨¡å¼)
                  å‘½ä»¤æ¨¡å¼ï¼šä½¿ç”¨ LLM è§£ææŒ‡ä»¤ï¼Œä½†ä¸¥æ ¼åªæ‰§è¡Œåœ°å›¾æ“ä½œ
                  å¯¹è¯æ¨¡å¼ï¼šä½¿ç”¨ LLM è‡ªç„¶å¯¹è¯ï¼Œå¯ä»¥é—²èŠä¹Ÿå¯ä»¥æ‰§è¡Œæ“ä½œ
            thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆæ·±åº¦æ¨ç†ï¼‰ï¼Œä¼šè¾“å‡º LLM çš„æ€è€ƒè¿‡ç¨‹

        Returns:
            {
                "message": "AI çš„å›å¤",
                "tool_call": { "action": ..., "arguments": ... } æˆ– None,
                "thinking": "æ€è€ƒè¿‡ç¨‹ï¼ˆä»… thinking=True æ—¶ï¼‰"
            }
        """
        if not self.use_llm or not self.llm_client:
            logger.warning("[ChatAssistant] LLM not available")
            return {
                "message": "âš ï¸ AI æœåŠ¡æœªå¯ç”¨ã€‚è¯·æ£€æŸ¥åç«¯é…ç½®æˆ–è”ç³»ç®¡ç†å‘˜ã€‚",
                "tool_call": None
            }

        # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ Function Callingï¼ˆä¸æ”¯æŒ thinking æ¨¡å¼æ—¶ï¼‰
        if self.use_function_calling and self._bridge and not thinking:
            logger.info("[ChatAssistant] Trying native Function Calling...")
            result = await self._chat_with_function_calling(user_input, mode)
            if result:
                logger.info("[ChatAssistant] Function Calling succeeded")
                return result
            logger.warning("[ChatAssistant] Function Calling failed, falling back to prompt-based")

        # ç­–ç•¥2: å›é€€åˆ° Prompt-based JSON å“åº”
        logger.info("[ChatAssistant] Using prompt-based approach...")

        # ç¡®å®š prompt key
        if mode == 'command':
            prompt_key = 'command_thinking' if thinking else 'command'
        else:
            prompt_key = 'conversation'

        # ä¼˜å…ˆä» MCP è·å– prompt
        mcp_prompt = await self._get_mcp_prompt(prompt_key)

        if mcp_prompt:
            # ä½¿ç”¨ MCP promptï¼ˆå·²åŒ…å«å®Œæ•´ä¿¡æ¯ï¼Œä¸éœ€è¦é¢å¤–æ³¨å…¥ï¼‰
            system_prompt = mcp_prompt
            logger.debug(f"[ChatAssistant] Using MCP prompt: {prompt_key}")
        else:
            # å›é€€åˆ°æœ¬åœ°ç¡¬ç¼–ç  prompt
            logger.info(f"[ChatAssistant] MCP prompt unavailable, using fallback: {prompt_key}")
            if mode == 'command':
                base_prompt = self.COMMAND_PROMPT_THINKING if thinking else self.COMMAND_PROMPT
            else:
                base_prompt = self.CONVERSATION_PROMPT
            # åŠ¨æ€æ³¨å…¥ MCP å·¥å…·åˆ—è¡¨ï¼ˆä»… fallback æ¨¡å¼éœ€è¦ï¼‰
            system_prompt = self._build_dynamic_prompt(base_prompt)

        result = await self._chat_with_llm(user_input, system_prompt, mode)
        if result:
            return result

        # LLM è°ƒç”¨å¤±è´¥
        logger.error("[ChatAssistant] LLM chat failed")
        return {
            "message": "âš ï¸ AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚",
            "tool_call": None
        }

    # ==================== Prompt-based LLM è°ƒç”¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰====================

    async def _chat_with_llm(self, user_input: str, system_prompt: str, mode: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ LLM è¿›è¡Œå¯¹è¯"""
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨ä¼ å…¥çš„ system_prompt
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # å¯¹è¯æ¨¡å¼ä¸‹æ·»åŠ å†å²å¯¹è¯ï¼ˆä¸Šä¸‹æ–‡ï¼‰ï¼Œå‘½ä»¤æ¨¡å¼ä¸éœ€è¦ä¸Šä¸‹æ–‡
        if mode == 'conversation':
            messages.extend(self.conversation_history)

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": user_input})

        try:
            # å‘½ä»¤æ¨¡å¼ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
            temperature = 0.3 if mode == 'command' else 0.7

            response = await self.llm_client.chat(
                messages=messages,
                temperature=temperature,
                max_tokens=1024,
                response_format={"type": "json_object"}
            )

            logger.info(f"[ChatAssistant] LLM response ({mode}): {response}")

            # è§£æ JSON å“åº”
            result = json.loads(response)

            # å¯¹è¯æ¨¡å¼ä¸‹ä¿å­˜åˆ°å†å²
            if mode == 'conversation':
                self.conversation_history.append(
                    {"role": "user", "content": user_input})
                self.conversation_history.append(
                    {"role": "assistant", "content": result.get("message", "")})

                # é™åˆ¶å†å²é•¿åº¦
                if len(self.conversation_history) > self.max_history * 2:
                    self.conversation_history = self.conversation_history[-self.max_history * 2:]

            return {
                "message": result.get("message", "..."),
                "tool_call": result.get("tool_call"),
                "thinking": result.get("thinking"),  # æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                "llm_raw": response  # æ·»åŠ  LLM åŸå§‹è¾“å‡ºç”¨äºè°ƒè¯•
            }

        except json.JSONDecodeError as e:
            logger.error(f"[ChatAssistant] JSON parse error: {e}")
            # å°è¯•ç›´æ¥è¿”å›æ–‡æœ¬
            return {
                "message": response if isinstance(response, str) else "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€ç‚¹é—®é¢˜ã€‚",
                "tool_call": None
            }
        except Exception as e:
            logger.error(f"[ChatAssistant] LLM error: {e}")
            return None

    # å…¼å®¹æ—§æ¥å£
    async def parse(self, user_input: str, mode: str = 'conversation') -> Optional[MCPToolCall]:
        """å…¼å®¹æ—§æ¥å£ï¼šè§£æç”¨æˆ·è¾“å…¥ï¼Œè¿”å›å·¥å…·è°ƒç”¨"""
        result = await self.chat(user_input, mode=mode)
        if result.get("tool_call"):
            tc = result["tool_call"]
            return MCPToolCall(
                id=str(uuid.uuid4()),
                action=tc["action"],
                arguments=tc.get("arguments", {})
            )
        return None

# ===================== WebSocket è¿æ¥ç®¡ç† =====================


class ConnectionManager:
    """WebSocket è¿æ¥ç®¡ç†å™¨"""

    def __init__(self):
        self.active_connections: List[WebSocket] = []
        # é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶æ˜¯å¦ä½¿ç”¨ LLM
        use_llm = os.getenv("USE_LLM", "false").lower() == "true"
        self.assistant = ChatAssistant(use_llm=use_llm)
        # å…¼å®¹æ—§ä»£ç 
        self.parser = self.assistant

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        print(
            f"[ConnectionManager] Client connected. Total: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        print(
            f"[ConnectionManager] Client disconnected. Total: {len(self.active_connections)}")

    async def send_action(self, websocket: WebSocket, tool_call: MCPToolCall):
        """å‘é€åŠ¨ä½œåˆ°å®¢æˆ·ç«¯"""
        await websocket.send_json({
            "type": "action",
            "id": tool_call.id,
            "payload": {
                "action": tool_call.action,
                "arguments": tool_call.arguments
            }
        })

    async def send_chat_response(self, websocket: WebSocket, message: str, tool_call: Optional[Dict] = None, llm_raw: Optional[str] = None, thinking: Optional[str] = None):
        """å‘é€å¯¹è¯å“åº”åˆ°å®¢æˆ·ç«¯"""
        response_data = {
            "type": "chat_response",
            "message": message,
            "timestamp": datetime.now().isoformat()
        }

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œé™„åŠ ä¸Šå»
        if tool_call:
            response_data["tool_call"] = {
                "id": str(uuid.uuid4()),
                "action": tool_call.get("action"),
                "arguments": tool_call.get("arguments", {})
            }

        # æ·»åŠ  LLM åŸå§‹è¾“å‡ºç”¨äºè°ƒè¯•
        if llm_raw:
            response_data["llm_raw"] = llm_raw

        # æ·»åŠ æ€è€ƒè¿‡ç¨‹
        if thinking:
            response_data["thinking"] = thinking

        await websocket.send_json(response_data)

    async def send_system(self, websocket: WebSocket, content: str):
        """å‘é€ç³»ç»Ÿæ¶ˆæ¯"""
        await websocket.send_json({
            "type": "system",
            "content": content,
            "timestamp": datetime.now().isoformat()
        })

    async def handle_message(self, websocket: WebSocket, data: Dict[str, Any]):
        """å¤„ç†å®¢æˆ·ç«¯æ¶ˆæ¯"""
        msg_type = data.get("type")

        if msg_type == "ping":
            await websocket.send_json({"type": "pong"})
            return

        if msg_type == "user_command":
            payload = data.get("payload", {})
            user_text = payload.get("text", "")
            mode = payload.get("mode", "conversation")  # é»˜è®¤å¯¹è¯æ¨¡å¼
            thinking = payload.get("thinking", False)   # æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼

            print(
                f"[ConnectionManager] Received message: {user_text} (mode: {mode}, thinking: {thinking})")

            # ä½¿ç”¨ ChatAssistant å¤„ç†ï¼Œä¼ å…¥ mode å’Œ thinking å‚æ•°
            result = await self.assistant.chat(user_text, mode=mode, thinking=thinking)

            # å‘é€å¯¹è¯å“åº”ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ã€LLM åŸå§‹è¾“å‡ºå’Œæ€è€ƒè¿‡ç¨‹ï¼‰
            # æ³¨æ„ï¼štool_call å·²åŒ…å«åœ¨ chat_response ä¸­ï¼Œå‰ç«¯ä¼šå¤„ç†æ‰§è¡Œ
            await self.send_chat_response(
                websocket,
                result.get("message", ""),
                result.get("tool_call"),
                result.get("llm_raw"),   # LLM åŸå§‹è¾“å‡º
                result.get("thinking")   # æ€è€ƒè¿‡ç¨‹
            )

            if result.get("tool_call"):
                tc = result["tool_call"]
                print(f"[ConnectionManager] Tool call: {tc['action']}({tc.get('arguments', {})})")

        if msg_type == "response":
            # å®¢æˆ·ç«¯è¿”å›çš„æ‰§è¡Œç»“æœ
            print(f"[ConnectionManager] Action response: {data}")

# ===================== FastAPI åº”ç”¨ =====================


manager = ConnectionManager()


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ GeoCommander Server starting...")

    # åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
    mcp_command = os.getenv("MCP_SERVER_COMMAND", "python -m mcp_geo_tools")
    print(f"ğŸ”Œ Connecting to MCP server: {mcp_command}")

    try:
        mcp_client = await init_mcp_client(mcp_command)
        if mcp_client.connected:
            print(f"âœ… MCP connected! {len(mcp_client.tools)} tools available")
            for tool in mcp_client.tools:
                print(f"   - {tool.name}")
        else:
            print("âš ï¸  MCP connection failed, using fallback mode")
    except Exception as e:
        print(f"âš ï¸  MCP initialization error: {e}")

    # é¢„çƒ­ Bridge èµ„æºç¼“å­˜
    bridge = get_bridge()
    locations = await bridge.get_locations()
    print(f"ğŸ“ MCP locations loaded: {len(locations)}")

    yield

    # æ–­å¼€ MCP è¿æ¥
    mcp_client = get_mcp_client()
    if mcp_client.connected:
        await mcp_client.disconnect()

    print("ğŸ‘‹ GeoCommander Server shutting down...")

app = FastAPI(
    title="GeoCommander MCP Server",
    description="åŸºäº MCP åè®®çš„è‡ªç„¶è¯­è¨€åœ°ç†ç©ºé—´æŒ‡ä»¤æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    """æœåŠ¡å™¨çŠ¶æ€"""
    # è·å– LLM æä¾›å•†ä¿¡æ¯
    llm_info = {"enabled": False, "provider": None}
    try:
        from llm_providers import provider_manager
        provider = provider_manager.get_active()
        if provider:
            llm_info = {
                "enabled": True,
                "provider": provider.name,
                "model": provider.model,
                "type": provider.type.value
            }
    except:
        pass

    # è·å– MCP çŠ¶æ€
    mcp_client = get_mcp_client()
    mcp_info = {
        "connected": mcp_client.connected,
        "tools": [t.name for t in mcp_client.tools] if mcp_client.connected else []
    }

    # è·å– locations æ•°é‡
    bridge = get_bridge()
    locations_count = len(await bridge.get_locations())

    return {
        "name": "GeoCommander Server",
        "version": "2.0.0",
        "status": "running",
        "mcp": mcp_info,
        "llm": llm_info,
        "locations_count": locations_count,
        "function_calling": True  # æ–°å¢: æ”¯æŒåŸç”Ÿ Function Calling
    }


@app.get("/tools")
async def get_tools():
    """è·å–æ‰€æœ‰ MCP å·¥å…·å®šä¹‰ï¼ˆä» MCP Server è·å–ï¼‰"""
    mcp_client = get_mcp_client()
    if mcp_client.connected:
        return {
            "tools": [
                {
                    "name": t.name,
                    "description": t.description,
                    "parameters": t.input_schema
                }
                for t in mcp_client.tools
            ]
        }
    # MCP æœªè¿æ¥æ—¶è¿”å›ç©ºåˆ—è¡¨
    return {"tools": [], "error": "MCP not connected"}


@app.get("/locations")
async def get_locations():
    """è·å–æ‰€æœ‰å·²çŸ¥åœ°ç‚¹ï¼ˆä» MCP èµ„æºè·å–ï¼‰"""
    bridge = get_bridge()
    locations = await bridge.get_locations()
    return {"locations": locations}


# ===================== MCP ç›¸å…³ç«¯ç‚¹ =====================

@app.get("/mcp/status")
async def mcp_status():
    """è·å– MCP å®¢æˆ·ç«¯çŠ¶æ€"""
    mcp_client = get_mcp_client()
    return {
        "connected": mcp_client.connected,
        "tools_count": len(mcp_client.tools) if mcp_client.connected else 0,
        "tools": [t.name for t in mcp_client.tools] if mcp_client.connected else []
    }


@app.get("/mcp/tools")
async def mcp_tools():
    """è·å– MCP å·¥å…·åˆ—è¡¨"""
    mcp_client = get_mcp_client()
    if not mcp_client.connected:
        return {"error": "MCP not connected", "tools": []}

    return {
        "tools": [
            {
                "name": t.name,
                "description": t.description,
                "parameters": t.input_schema
            }
            for t in mcp_client.tools
        ]
    }


@app.get("/mcp/resources")
async def mcp_resources():
    """è·å– MCP èµ„æºåˆ—è¡¨"""
    mcp_client = get_mcp_client()
    if not mcp_client.connected:
        return {"error": "MCP not connected", "resources": []}

    resources = await mcp_client.get_resources()
    return {"resources": resources}


@app.get("/mcp/prompts")
async def mcp_prompts():
    """è·å– MCP æç¤ºè¯åˆ—è¡¨"""
    mcp_client = get_mcp_client()
    if not mcp_client.connected:
        return {"error": "MCP not connected", "prompts": []}

    prompts = await mcp_client.get_prompts()
    return {"prompts": prompts}


class MCPToolCallRequest(BaseModel):
    """MCP å·¥å…·è°ƒç”¨è¯·æ±‚"""
    tool: str
    arguments: Dict[str, Any] = {}
    broadcast: bool = True  # æ˜¯å¦å¹¿æ’­åˆ°å‰ç«¯


@app.post("/mcp/call")
async def mcp_call_tool(request: MCPToolCallRequest):
    """
    è°ƒç”¨ MCP å·¥å…·

    è¿™æ˜¯æµ‹è¯• MCP å·¥å…·çš„ä¸»è¦ç«¯ç‚¹ã€‚
    è°ƒç”¨å·¥å…·åï¼Œå¦‚æœ broadcast=Trueï¼Œä¼šå°†ç»“æœå¹¿æ’­åˆ°å·²è¿æ¥çš„å‰ç«¯ã€‚

    ç¤ºä¾‹è¯·æ±‚:
    POST /mcp/call
    {
        "tool": "fly_to_location",
        "arguments": {"name": "åŒ—äº¬"},
        "broadcast": true
    }
    """
    mcp_client = get_mcp_client()

    if not mcp_client.connected:
        return {
            "success": False,
            "error": "MCP not connected"
        }

    # è°ƒç”¨ MCP å·¥å…·
    result = await mcp_client.call_tool(request.tool, request.arguments)

    logger.info(f"[MCP Call] {request.tool}({request.arguments}) -> {result}")

    # å¦‚æœéœ€è¦å¹¿æ’­åˆ°å‰ç«¯
    if request.broadcast and result.get("action"):
        tool_call = MCPToolCall(
            id=str(uuid.uuid4()),
            action=result.get("action"),
            arguments=result.get("arguments", {})
        )

        # å¹¿æ’­åˆ°æ‰€æœ‰å·²è¿æ¥çš„å®¢æˆ·ç«¯
        for ws in manager.active_connections:
            try:
                await manager.send_action(ws, tool_call)
            except Exception as e:
                logger.warning(f"[MCP Call] Failed to broadcast: {e}")

        result["broadcasted"] = True
        result["clients"] = len(manager.active_connections)

    return result


@app.get("/model")
async def get_current_model():
    """è·å–å½“å‰ä½¿ç”¨çš„ LLM æ¨¡å‹"""
    try:
        from llm_providers import provider_manager
        provider = provider_manager.get_active()
        if provider:
            return {
                "model": provider.model,
                "provider": provider.name,
                "type": provider.type.value
            }
        return {"model": None, "provider": None}
    except Exception as e:
        return {"model": None, "error": str(e)}


@app.get("/providers")
async def get_providers():
    """è·å–æ‰€æœ‰ LLM æœåŠ¡å•†"""
    try:
        from llm_providers import provider_manager, check_ollama_available, get_ollama_models

        providers = provider_manager.list_providers()

        # æ£€æŸ¥ Ollama çŠ¶æ€
        ollama_available = await check_ollama_available()
        ollama_models = await get_ollama_models() if ollama_available else []

        return {
            "providers": providers,
            "ollama": {
                "available": ollama_available,
                "models": ollama_models
            }
        }
    except Exception as e:
        return {
            "providers": [],
            "error": str(e)
        }


@app.post("/providers/select")
async def select_provider(body: Dict[str, Any]):
    """é€‰æ‹©æœåŠ¡å•†å’Œæ¨¡å‹"""
    try:
        from llm_providers import provider_manager

        provider_name = body.get("provider")
        model = body.get("model")

        if provider_name:
            provider_manager.set_active(provider_name)
            logger.info(f"[API] Switched to provider: {provider_name}")

        if model and provider_name:
            provider_manager.set_model(provider_name, model)
            logger.info(f"[API] Set model: {model}")

        # é‡æ–°åˆå§‹åŒ– parser çš„ LLM å®¢æˆ·ç«¯
        manager.parser.llm_client = provider_manager.get_client()

        active = provider_manager.get_active()
        return {
            "success": True,
            "active_provider": active.name if active else None,
            "model": active.model if active else None
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    message: str
    system_prompt: Optional[str] = None


class ExecuteRequest(BaseModel):
    """æ‰§è¡Œè¯·æ±‚ - ç”¨äº MCP Server è¿œç¨‹æ‰§è¡Œ"""
    action: str
    arguments: Dict[str, Any] = {}


@app.post("/execute")
async def execute_action(request: ExecuteRequest):
    """
    æ‰§è¡ŒåŠ¨ä½œç«¯ç‚¹ - ä¾› MCP Server è¿œç¨‹è°ƒç”¨

    æ¥æ”¶æ¥è‡ª mcp-geo-tools çš„åŠ¨ä½œå‘½ä»¤ï¼Œå¹¿æ’­åˆ°å·²è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯ã€‚
    è¿™ä½¿å¾— MCP Server å¯ä»¥é€šè¿‡ HTTP ç›´æ¥æ§åˆ¶ Cesium å‰ç«¯ã€‚

    Args:
        request: åŒ…å« action åç§°å’Œ arguments å‚æ•°çš„è¯·æ±‚ä½“

    Returns:
        æ‰§è¡Œç»“æœï¼ŒåŒ…æ‹¬æˆåŠŸçŠ¶æ€å’Œå·²é€šçŸ¥çš„å®¢æˆ·ç«¯æ•°é‡
    """
    try:
        # åˆ›å»ºå·¥å…·è°ƒç”¨å¯¹è±¡
        tool_call = MCPToolCall(
            id=str(uuid.uuid4()),
            action=request.action,
            arguments=request.arguments
        )

        # å¹¿æ’­åˆ°æ‰€æœ‰å·²è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯
        connected_count = len(manager.active_connections)

        if connected_count == 0:
            return {
                "success": False,
                "error": "No connected clients",
                "message": "æ²¡æœ‰å·²è¿æ¥çš„å®¢æˆ·ç«¯ï¼Œè¯·ç¡®ä¿ Cesium å‰ç«¯å·²æ‰“å¼€å¹¶è¿æ¥"
            }

        # å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€åŠ¨ä½œ
        for websocket in manager.active_connections:
            try:
                await manager.send_action(websocket, tool_call)
            except Exception as e:
                logger.warning(f"Failed to send action to client: {e}")

        logger.info(f"[Execute API] Executed {request.action} to {connected_count} clients")

        return {
            "success": True,
            "action": request.action,
            "arguments": request.arguments,
            "clients_notified": connected_count,
            "message": f"åŠ¨ä½œå·²å‘é€åˆ° {connected_count} ä¸ªå®¢æˆ·ç«¯"
        }

    except Exception as e:
        logger.error(f"[Execute API] Error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/chat")
async def chat(request: ChatRequest):
    """
    ç®€å•å¯¹è¯æ¥å£ - æµ‹è¯• LLM è¿æ¥

    ä¸æ¶‰åŠ MCP å·¥å…·è°ƒç”¨ï¼Œåªæ˜¯çº¯ç²¹çš„ LLM å¯¹è¯
    """
    try:
        from llm_providers import provider_manager

        client = provider_manager.get_client()
        if not client:
            return {
                "success": False,
                "error": "No LLM provider available"
            }

        provider = provider_manager.get_active()

        messages = []
        if request.system_prompt:
            messages.append(
                {"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.message})

        try:
            response = await client.chat(messages)
            return {
                "success": True,
                "provider": provider.name if provider else "unknown",
                "model": provider.model if provider else "unknown",
                "response": response
            }
        finally:
            await client.close()

    except Exception as e:
        logger.error(f"[Chat] Error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket ç«¯ç‚¹"""
    await manager.connect(websocket)

    try:
        # å‘é€æ¬¢è¿æ¶ˆæ¯
        await manager.send_system(
            websocket,
            "å·²è¿æ¥åˆ° GeoCommander MCP Serverã€‚æ‚¨å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æ§åˆ¶åœ°å›¾ï¼Œä¾‹å¦‚ï¼š'é£åˆ°ä¸Šæµ·å¤–æ»©'ã€‚"
        )

        while True:
            data = await websocket.receive_json()
            await manager.handle_message(websocket, data)

    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        print(f"[WebSocket] Error: {e}")
        manager.disconnect(websocket)

# ===================== å¯åŠ¨å…¥å£ =====================

if __name__ == "__main__":
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8765,
        reload=True,
        log_level="info"
    )
